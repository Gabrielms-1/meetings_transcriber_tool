import json
from contextlib import asynccontextmanager
from transformers import AutoTokenizer, AutoModelForCausalLM
from fastapi import FastAPI, UploadFile, File, HTTPException, Request
from fastapi.responses import PlainTextResponse
import torch

@asynccontextmanager
async def lifespan(app: FastAPI):
    model_name = "Qwen/Qwen2.5-7B-Instruct-AWQ"
    app.state.model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype="auto",
        device_map="auto"
    )
    app.state.tokenizer = AutoTokenizer.from_pretrained(model_name)
    yield



app = FastAPI(lifespan=lifespan)


def summarize_dialogue(model, tokenizer, dialogue: dict) -> str:
    """
    dialogue: JSON dictionary with the dialogue
    Returns the markdown generated by the model.
    """
    prompt = (
        "You are a helpful assistant that summarizes and organizes a conversation. The conversation is separated by the turns of each participant, being these separated by Speaker letters (A, B, C, etc.).:\n\n"
        + json.dumps(dialogue, ensure_ascii=False, indent=2)
        + "\n\nPlease generate a long text in markdown that synthesizes and organizes this conversation. I want to read a long text that explains what happened in the meeting and what decisions were made, then a paragraph with the next steps."
    )

    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    output_ids = model.generate(**inputs, max_new_tokens=1024)
    markdown = tokenizer.decode(output_ids[0], skip_special_tokens=True)
    return markdown


@app.post("/summarize", response_class=PlainTextResponse)
async def infer(request: Request, file: UploadFile = File(...)):
    if not file.filename.endswith(".json"):
        raise HTTPException(status_code=415, detail="Only .json files are supported")
    content = await file.read()
    try:
        dialogue = json.loads(content)
    except json.JSONDecodeError:
        raise HTTPException(status_code=400, detail="Invalid JSON")

    markdown = summarize_dialogue(request.app.state.model, request.app.state.tokenizer, dialogue)
    return markdown


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000, reload=True)
